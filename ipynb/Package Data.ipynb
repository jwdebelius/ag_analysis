{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Justine Debelius<br>\n",
    "**email**: jdebelius@ucsd.edu<br>\n",
    "**enviroment**: qiime2-2017.4<br>\n",
    "**Date**: 8 May 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assemble a processing package for American Gut data. We'll build a set of directories which contain the following set of files:\n",
    "\n",
    "**Metadata**<br>\n",
    "The sample and prep metadata downloaded from Qiita, along with appended Vioscreen results.\n",
    "Alpha diversity (PD whole tree, shannon, and observed OTUs) for the rarefaction depth, and every depth lower have been added.\n",
    "\n",
    "**OTU table**<br>\n",
    "A rarefied and unrarefied biom table are provided. The rarefied table is denoted by the rarefaction depth. The unrarefied table is filtered for samples with fewer than the number of reads denoted by the rarefaction level (e.g., all samples with fewer than 1250 sequences are removed from the unrarefied 1250 biom table).\n",
    "\n",
    "**Distance Matrices**<br>\n",
    "The weighted, normalized-weighted, unweighted UniFrac and Bray-Curtis distance are provided.\n",
    "\n",
    "**PICRUSt**<br>\n",
    "PICRUSt prediction based on clustering the deblur sequences against the greengenes 13_8 OTU database at 99% and then performing PICRUSt prediction. Tables were filtered to remove samples with fewer than 1250 sequences/sample before normalization for 16S copy number. Tables collapsed at L1, L2, and L3 are also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import biom\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the depths and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depths = ['1250', '2500', '5000', '10000', '50000']\n",
    "alpha_metrics = ['observed_otus', 'faiths_pd', 'shannon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates directories for the packaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for depth in depths:\n",
    "    dir_path = './03.packaged/%s' % depth\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the unrarefied table, since rarefaction strips the taxonommy, so this can be added back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_t = load_table('./03.packaged/1250/deblur_125nt_no_blooms.biom')\n",
    "taxa_lookup = {id_: {'taxonomy': raw_t.metadata(id_, axis='observation')['taxonomy']}\n",
    "               for id_ in raw_t.ids(axis='observation')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll walk through the tables, add taxonomy, and save them to the new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for depth in depths:\n",
    "    # Sets up the file paths\n",
    "    old_table_fp = './02.build_package/02.rarefied/%s/deblur_125nt_0.qza' % depth\n",
    "    new_table_fp = './03.packaged/%s/deblur_125nt_no_blooms_rare.biom' % (depth)\n",
    "    temp_dir = './table_temp'\n",
    "    \n",
    "    # Extracts the feature table into a biom table\n",
    "    !qiime tools export $old_table_fp --output-dir $temp_dir\n",
    "    \n",
    "    table_ = biom.load_table(os.path.join(temp_dir, 'feature-table.biom'), new_table_fp)\n",
    "    table_.add_metadata(taxa_lookup, axis='observation')\n",
    "    \n",
    "    with biom.util.biom_open(new_table_fp, 'r'):\n",
    "        table_.to_hdf5(f_, 'rarefied with taxa')\n",
    "    \n",
    "    os.removedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next load metadata and combine it with the alpha diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_ = pd.read_csv('./02.build_package/01.input/fecal_map_1250.txt', sep='\\t', dtype=str)\n",
    "map_.set_index('#SampleID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_.drop(['index'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequencing depth, starting with the shallowest rarefaction depth, we'll append the diversity to the mapping file and then filter to remove only samples that contain those samples. \n",
    "\n",
    "To do this, we need to export the alpha diversity artifacts, since the QIIME 2 python API doesn't handle the American Gut barcodes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_ids = {}\n",
    "for depth in ['1250', '2500', '5000', '10000', '50000']:\n",
    "# for depth in ['50000']:\n",
    "    collated = []\n",
    "    for metric in ['observed_otus', 'faiths_pd', 'shannon']:\n",
    "#     for metric in ['faiths_pd']:\n",
    "        alpha_ = []\n",
    "        for i in np.arange(0, 10):\n",
    "            alpha_artifact = './02.build_package/03.alpha/%s/%s_r%i.qza' % (depth, metric, i)\n",
    "            alpha_dir = './02.build_package/03.alpha/%s/%s/' % (depth, metric)\n",
    "            \n",
    "            !qiime tools export $alpha_artifact --output-dir $alpha_dir\n",
    "            \n",
    "            alpha_.append(pd.read_csv(os.path.join(alpha_dir, 'alpha-diversity.tsv'),\n",
    "                                      sep='\\t', dtype=str\n",
    "                                     ).rename(columns={'Unnamed: 0': '#SampleID', \n",
    "                                                       metric: '%s_%s_%i' % (metric, depth, i)}\n",
    "                                             ).set_index('#SampleID'))\n",
    "        alpha_ = pd.concat(alpha_, axis=1).astype(float)\n",
    "        collated.append(alpha_)\n",
    "        map_.loc[alpha_.index, '%s_%s' % (metric, depth)] = alpha_.mean(1)\n",
    "    collated = pd.concat(collated, axis=1)\n",
    "    collated.to_csv('./03.packaged/%s/collated_alpha.txt' % depth, sep='\\t', index_label='#SampleID')\n",
    "    sample_ids[depth] = collated.index\n",
    "    map_.dropna().to_csv('./03.packaged/%s/ag_map_with_alpha.txt' % depth,\n",
    "                         sep='\\t', index_label='#SampleID')\n",
    "    with open('./03.packaged/%s/sample_id.txt' % depth, 'w') as f_:\n",
    "        f_.write('\\n'.join(collated.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the list of sample IDs and then filter the unrarefied deblur biom tables and picrust tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "picrust_path = './02.raw_tables/picrust/otu_table_no_blooms_125nt_with_tax_min1250_gg99_normed_pred%s.biom'\n",
    "picrust_tables = {\n",
    "#     'raw': biom.load_table(picrust_path % ''),\n",
    "    '_L1': biom.load_table(picrust_path % '_L1'),\n",
    "    '_L2': biom.load_table(picrust_path % '_L2'),\n",
    "    '_L3': biom.load_table(picrust_path % '_L3'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "picrust_out = './03.packaged/%s/picrust/deblur_no_blooms_125nt_min1250_gg99_normed_pred%s.biom'\n",
    "\n",
    "for depth in ['1250', '2500', '5000', '10000', '50000']:\n",
    "    sample_path = './03.packaged/%s/sample_id.txt' % depth\n",
    "    # Filters the unrarefied biom table to the appropriate depth\n",
    "    !biom subset-table \\\n",
    "        --input-hdf5-fp ./02.build_package/01.input/deblur_no_blooms_125nt_1250.biom \\\n",
    "        --ids $sample_path \\\n",
    "        --axis sample \\\n",
    "        --output-fp ./03.packaged/$depth/deblur_125nt_no_blooms.biom\n",
    "    \n",
    "    # Creates a directory with the PICRUSt data\n",
    "    if not os.path.exists('./03.packaged/%s/picrust' % depth):\n",
    "        os.makedirs('./03.packaged/%s/picrust' % depth)\n",
    "\n",
    "    # Filters the picrust tables and puts them in the directory\n",
    "    for collapse in ['_L1', '_L2', '_L3']:\n",
    "        picrust_fp_in = picrust_path % collapse\n",
    "        picrust_fp_out = picrust_out % (depth, collapse)\n",
    "        !biom subset-table \\\n",
    "            --input-hdf5-fp $picrust_fp_in \\\n",
    "            --output-fp $picrust_fp_out \\\n",
    "            --ids $sample_path \\\n",
    "            --axis 'sample'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from biom.table import vlen_list_of_str_formatter\n",
    "\n",
    "from json import dumps, loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def picrust_parser(*args):\n",
    "    new_value = []\n",
    "    for arg in args:\n",
    "         new_value.append(loads(arg[0]))\n",
    "    return new_value if new_value else None\n",
    "\n",
    "\n",
    "def picrust_formatter(*args):\n",
    "    \"\"\"Transform, and format\n",
    "    \n",
    "    Taken directly from\n",
    "    https://github.com/picrust/picrust/blob/master/picrust/util.py#L474\n",
    "    per Daniel McDonald's directions to get the complex PICRUSt data\n",
    "    to work...\n",
    "    \"\"\"\n",
    "    return vlen_list_of_str_formatter(*list_of_list_of_str_formatter(*args))\n",
    "\n",
    "def list_of_list_of_str_formatter(grp, header, md, compression):\n",
    "    \"\"\"Serialize [[str]] into a BIOM hdf5 compatible form\n",
    "    Parameters\n",
    "    ----------\n",
    "    grp : h5py.Group\n",
    "        This is ignored. Provided for passthrough\n",
    "    header : str\n",
    "        The key in each dict to pull out\n",
    "    md : list of dict\n",
    "        The axis metadata\n",
    "    compression : bool\n",
    "        Whether to enable dataset compression. This is ignored, provided for\n",
    "        passthrough\n",
    "    Returns\n",
    "    -------\n",
    "    grp : h5py.Group\n",
    "        The h5py.Group\n",
    "    header : str\n",
    "        The key in each dict to pull out\n",
    "    md : list of dict\n",
    "        The modified metadata that can be formatted in hdf5\n",
    "    compression : bool\n",
    "        Whether to enable dataset compression.\n",
    "    Notes\n",
    "    -----\n",
    "    This method is intended to be a \"passthrough\" to BIOM's\n",
    "    vlen_list_of_str_formatter method. It is a transform method.\n",
    "    \"\"\"\n",
    "    new_md = [{header: np.atleast_1d(np.asarray(dumps(m[header])))} for m in md]\n",
    "    return (grp, header, new_md, compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "format_fs = {'KEGG_Description': picrust_formatter,\n",
    "             'COG_Description': picrust_formatter,\n",
    "             'KEGG_Pathways': picrust_formatter,\n",
    "             'COG_Category': picrust_formatter\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parse_fs = {'KEGG_Pathways': picrust_parser, 'KEGG_Description': picrust_parser}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp_in = './02.raw_tables/picrust/otu_table_no_blooms_125nt_with_tax_min1250_gg99_normed_pred.biom'\n",
    "with h5py.File(fp_in) as f_:\n",
    "    t_in = biom.Table.from_hdf5(f_, parse_fs=parse_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for depth in depths[::-1]:\n",
    "    print(depth)\n",
    "    id_ = sample_ids[depth]\n",
    "    picrust_out = './03.packaged/%s/picrust/deblur_no_blooms_125nt_min1250_gg99_normed_pred.biom' % depth\n",
    "    with h5py.File(fp_in) as f_:\n",
    "        t_in = biom.Table.from_hdf5(f_, parse_fs=parse_fs)\n",
    "    \n",
    "    t_depth = t_in.filter(id_, axis='sample', inplace=False)\n",
    "    \n",
    "    with h5py.File(picrust_out, 'w') as fp_:\n",
    "        t_depth.to_hdf5(fp_, 'filtered for %s sequences/sample' % depth, format_fs=format_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll move the beta diversity files over, from the beta folder. To do this, we'll extract the data, and then move the table into the appropriate directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for depth in depths:\n",
    "for depth in depths:\n",
    "    temp_dir = './02.build_package/04.beta/%s/temp_dir/' % depth\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    if not os.path.exists('./03.packaged/%s/distance' % depth):\n",
    "        os.makedirs('./03.packaged/%s/distance' % depth)\n",
    "    for metric in ['bray_curtis', 'unweighted', 'weighted-normalized', 'weighted-unnormalized']:\n",
    "#     for metric in ['bray_curtis']:\n",
    "        old_filepath = './02.build_package/04.beta/%s/%s.qza' % (depth, metric)\n",
    "        new_path = './03.packaged/%s/distance/%s.txt'\n",
    "        !qiime tools extract \\\n",
    "            $old_filepath \\\n",
    "            --output-dir $temp_dir\n",
    "        uuid_dir = os.path.join(temp_dir, os.listdir(temp_dir)[0])\n",
    "        full_fp = os.path.join(uuid_dir, 'data/distance-matrix.tsv')\n",
    "        shutil.move(full_fp, new_path % (depth, metric))\n",
    "        !rm -r $uuid_dir\n",
    "    os.removedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
